---
layout:		post
title:      对话情绪识别学习笔记
subtitle:	
date:       2020-10-16
author:     xuelin
header-img: img/post-web.jpg
catalog:    true
tags:
    - Dialogue
---

# 背景

情感描述符

1、三维的Valence效价-Arousal唤醒-Dominance支配
v：pleasure愉快 vs displeasure不愉快
a：low engagement低参与感 vs ecstasy狂热
d：extent of control 控制程度

2、plutchik普拉切克感情车轮
8个原始分类：生气、恐惧、厌恶、信任、难过、开心、期待、惊讶
3个层次圈：代表和周围情绪的相似程度

# 任务介绍

对话情绪识别是一个分类任务，旨在对一段对话中的话语进行情绪分类。任务的输入是一段连续的对话，输出是这段对话中所有话语的情绪，下图给出了一个简单的示例。

![](/assets/16028303123620.jpg)

对话存在三个特点：1、对话文本是短的、非正式文本；2、对话中的主题时常快速切换，因此上下文是动态的；3、对话者之间的交互会改变用户的情绪和状态。

由于对话本身具有很多要素，话语的情绪识别并不简单等同于单个句子的情绪识别，而是需要综合考虑对话中的背景、上下文、说话人等信息，这些都是对话情绪识别任务中独特的挑战。

##### 挑战

* 交互方面：说话人的差异；听众的反应；多方对话
* 情感动态方面：动态上下文；情感惯性&人际影响
* 话题方面：细粒度的情感；讽刺挖苦（machine比较难领会）

# 数据集介绍

[IEMOCAP](https://sail.usc.edu/iemocap/)

南加州大学的SAIL实验室收集，由人扮演的双人对话，12小时的多模态视听数据。10个专业演员（5男5女），一共分为5个Session，每个Session分配1男1女。对话分为两部分，一部分是固定的剧本，另一部分是给定主题情景下的自由发挥。151段对话，共7433句。标注了6类情绪：Neutral, Happiness, Sadness, Anger, Frustrated, Excited，非中性情绪占比77%。IEMOCAP是对话情绪识别中最常用的数据集，质量较高，优点是有多模态信息，缺点是数据规模较小。

[SEMAINE](https://semaine-db.eu/)

SEMAINE数据库收集的多模态对话数据，由四个固定形象的机器人与人进行对话，曾用于AVEC2012挑战赛。AVEC2012使用的数据有95段对话，共5798句。标注了4个情感维度：Valence (愉悦度), Arousal (激活度), Expectancy (预期), Power (力量)。Valence表示情感积极的程度，Arousal表示兴奋的程度，Expectancy表示与预期相符的程度，Power表示情感影响力。其中Valence、Arousa和Expectancy为[-1, 1]范围内的连续值，Power为大于等于0的连续值。SEMAINE是对话情绪识别中常用的数据集之一，缺点是数据规模较小。

[DailyDialog](http://yanran.li/dailydialog)

高质量多轮对话数据集，纯文本，噪声小，对话反映不同主题的日常生活，无固定说话人。数据集除了7类情绪标注，还有10类主题标注以及4类对话行为标注。12218段对话，共103607句。标注了7类情绪：Neutral, Happiness, Surprise, Sadness, Anger, Disgust, Fear，非中性情绪占比16.8%。DailyDialog在对话情绪识别中应用较少，优点是数据规模较大，缺点是中性情绪占比过高。

[EmotionLines](http://doraemon.iis.sinica.edu.tw/emotionlines/index.html)

来源于老友记（多人对话）和私人Facebook聊天记录（双人对话），纯文本，有固定说话人。曾用于SocialNLP 2018 EmotionX Challenge。内容上两部分独立，各1000段对话，共29245句。标注了7类情绪：Neutral, Happiness, Surprise, Sadness, Anger, Disgust, Fear，非中性情绪占比44.5%。EmotionLines在对话情绪识别中应用较少，通常使用多模态的MELD数据集。

[EmoContext](https://www.humanizing-ai.com/emocontext.html)

纯文本双人对话，每段对话三句，仅最后一句有情感标签。用于SemEval-2019 Task 3。38421段对话，共115263句。标注了4类情绪：Happiness, Sadness, Anger, Other，非中性情绪占比42.8%。EmoContext在对话情绪识别中应用较少，优点是数据规模较大，缺点是对话长度过短和仅标注最后一句。

[MELD](https://affective-meld.github.io/)

来源于老友记，多人对话形式，是EmotionLines老友记部分的多模态扩充（文本+视频）。1433段对话，共13708句。标注了7类情绪：Neutral, Happiness, Surprise, Sadness, Anger, Disgust, Fear和3类情感：Positive, Negative, Neutral，非中性情绪占比53%。MELD是对话情绪识别中常用的数据集之一，优点是数据集质量较高并且有多模态信息，缺点是数据集中的对话涉及到的剧情背景太多，情绪识别难度很大。


![](/assets/16029398314753.jpg)

# 相关工作介绍

第一类：上下文建模

区别于传统的单句情感分析，对话中的话语进行分类时，周围的话语可以提供重要的上下文信息。C-LSTM是基于LSTM的能够捕捉上下文信息的模型，其模型如图所示。每个话语的输入特征经过LSTM单元以及一个全连接层得到话语的输出特征，用于话语情感分类。双向的LSTM可捕捉话语的上下文关系，优于单向LSTM。

![](/assets/16028308821926.jpg)

第二类：说话人建模

对话中除了话语的上下文信息外，还需要考虑说话人的状态与相互影响。CMN对于当前要识别的话语，通过GRU分别建模每个说话人的历史话语，作为记忆单元。然后通过注意力机制将每个说话人的记忆与当前话语的表示进行融合，得到的结果用于话语分类，从而模拟说话人个人的状态以及不同说话人的状态对当前话语的影响，其模型如图所示。

![](/assets/16028309433884.jpg)

CMN对于不同的说话人采用了独立的记忆单元，在此基础之上，ICON使用交互式的记忆单元，其模型如下图所示。对于当前要识别的话语，ICON通过SIM(Self-Influence Module)模块分别对每个说话人的历史话语进行建模，接着通过DGIM(Dynamic Global Influence Module)模块对说话人之间的影响进行建模得到全局的状态，存入记忆单元，然后使用Attention机制得到记忆单元与当前话语表示的融合结果，用于话语分类。

![](/assets/16028310275794.jpg)


第三类：区分说话人的建模

CMN、ICON等模型虽然对不同的说话人信息进行了建模，但是对于最终要识别的话语，并未区分该话语属于哪个说话人。DialogueRNN解决了这个问题，同时认为对话中的话语情感取决于三个因素：说话人信息，先前话语的语境和情感信息，并分别使用说话人状态(Party GRU)、全局状态(Global GRU)和情感状态(Emotion GRU)进行捕捉，其模型如图所示。对于当前时刻的话语，全局状态由前一时刻的全局状态、当前话语的表示、当前话语说话人前一时刻的状态进行更新，说话人状态由当前话语说话人前一时刻的状态、当前话语的表示、之前时刻的全局状态进行更新，情感状态由说话人当前时刻的状态以及上一时刻的情感状态更新，之后用当前时刻的情感状态进行当前话语的分类。

![](/assets/16028310905538.jpg)

第四类：和Dialog Act识别联合建模

在对话中，情感识别和dialog act识别两个任务是紧密联系的，考虑显示建模这两个任务的关系能够同时促进两个任务的性能。DCR-Net首次显示的探索建模了两者的交互，并且系统的对比了三种不同的关系层。最终在多个数据集上证明了联合建模的有效性。

![](/assets/16028311089819.jpg)
